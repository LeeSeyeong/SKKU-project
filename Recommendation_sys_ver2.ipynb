{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bU_1V_r1T4FDx_wytV1BF2GiiTQT8POJ",
      "authorship_tag": "ABX9TyMNJXNjVzu326DfFlONOUi0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVyNVemxTNJB",
        "outputId": "4e95c904-ce5d-4fc3-fe0b-7873647d1b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " pip install fuzzywuzzy pip install python-Levenshtein  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj8Hv5baHqo9",
        "outputId": "0efe1886-0af2-4d52-c912-b1b96501f08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting install\n",
            "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.20.7-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.20.7\n",
            "  Downloading Levenshtein-0.20.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 65.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein, install, fuzzywuzzy\n",
            "Successfully installed Levenshtein-0.20.7 fuzzywuzzy-0.18.0 install-1.3.5 python-Levenshtein-0.20.7 rapidfuzz-2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUWrRgoJ7sX0",
        "outputId": "fff4809d-fde6-484e-9f69-f657c05e99c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.7.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1633980 sha256=f6abdd96101a306985980586c6eb4a63537c33ea36a410b28a18dd87ad90d0d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class AutoRec(object):\n",
        "\n",
        "    def __init__(self, visibleDimensions, epochs=200, hiddenDimensions=50, learningRate=0.1, batchSize=100):\n",
        "\n",
        "        self.visibleDimensions = visibleDimensions\n",
        "        self.epochs = epochs\n",
        "        self.hiddenDimensions = hiddenDimensions\n",
        "        self.learningRate = learningRate\n",
        "        self.batchSize = batchSize\n",
        "        self.optimizer = tf.keras.optimizers.RMSprop(self.learningRate)\n",
        "\n",
        "    def Train(self, X):\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            for i in range(0, X.shape[0], self.batchSize):\n",
        "                epochX = X[i:i+self.batchSize]\n",
        "                self.run_optimization(epochX)\n",
        "\n",
        "            print(\"Trained epoch \", epoch)\n",
        "\n",
        "    def GetRecommendations(self, inputUser):\n",
        "\n",
        "        # Feed through a single user and return predictions from the output layer.\n",
        "        rec = self.neural_net(inputUser)\n",
        "\n",
        "        # It is being used as the return type is Eager Tensor.\n",
        "        return rec[0]\n",
        "\n",
        "    def neural_net(self, inputUser):\n",
        "\n",
        "        #tf.set_random_seed(0)\n",
        "\n",
        "        # Create varaibles for weights for the encoding (visible->hidden) and decoding (hidden->output) stages, randomly initialized\n",
        "        self.weights = {\n",
        "            'h1': tf.Variable(tf.random.normal([self.visibleDimensions, self.hiddenDimensions])),\n",
        "            'out': tf.Variable(tf.random.normal([self.hiddenDimensions, self.visibleDimensions]))\n",
        "            }\n",
        "\n",
        "        # Create biases\n",
        "        self.biases = {\n",
        "            'b1': tf.Variable(tf.random.normal([self.hiddenDimensions])),\n",
        "            'out': tf.Variable(tf.random.normal([self.visibleDimensions]))\n",
        "            }\n",
        "\n",
        "        # Create the input layer\n",
        "        self.inputLayer = inputUser\n",
        "\n",
        "        # hidden layer\n",
        "        hidden = tf.nn.sigmoid(tf.add(tf.matmul(self.inputLayer, self.weights['h1']), self.biases['b1']))\n",
        "\n",
        "        # output layer for our predictions.\n",
        "        self.outputLayer = tf.nn.sigmoid(tf.add(tf.matmul(hidden, self.weights['out']), self.biases['out']))\n",
        "\n",
        "        return self.outputLayer\n",
        "\n",
        "    def run_optimization(self, inputUser):\n",
        "        with tf.GradientTape() as g:\n",
        "            pred = self.neural_net(inputUser)\n",
        "            loss = tf.keras.losses.MSE(inputUser, pred)\n",
        "\n",
        "        trainable_variables = list(self.weights.values()) + list(self.biases.values())\n",
        "\n",
        "        gradients = g.gradient(loss, trainable_variables)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "from surprise import AlgoBase\n",
        "from surprise import PredictionImpossible\n",
        "import numpy as np\n",
        "\n",
        "class AutoRecAlgorithm(AlgoBase):\n",
        "\n",
        "    def __init__(self, epochs=100, hiddenDim=100, learningRate=0.01, batchSize=100, sim_options={}):\n",
        "        AlgoBase.__init__(self)\n",
        "        self.epochs = epochs\n",
        "        self.hiddenDim = hiddenDim\n",
        "        self.learningRate = learningRate\n",
        "        self.batchSize = batchSize\n",
        "\n",
        "    def fit(self, trainset):\n",
        "        AlgoBase.fit(self, trainset)\n",
        "\n",
        "        numUsers = trainset.n_users\n",
        "        numItems = trainset.n_items\n",
        "\n",
        "        trainingMatrix = np.zeros([numUsers, numItems], dtype=np.float32)\n",
        "\n",
        "        for (uid, iid, rating) in trainset.all_ratings():\n",
        "            trainingMatrix[int(uid), int(iid)] = rating / 5.0\n",
        "\n",
        "        # Create an RBM with (num items * rating values) visible nodes\n",
        "        autoRec = AutoRec(trainingMatrix.shape[1], hiddenDimensions=self.hiddenDim, learningRate=self.learningRate, batchSize=self.batchSize, epochs=self.epochs)\n",
        "        autoRec.Train(trainingMatrix)\n",
        "\n",
        "        self.predictedRatings = np.zeros([numUsers, numItems], dtype=np.float32)\n",
        "\n",
        "        for uiid in range(trainset.n_users):\n",
        "            if (uiid % 50 == 0):\n",
        "                print(\"Processing user \", uiid)\n",
        "            recs = autoRec.GetRecommendations([trainingMatrix[uiid]])\n",
        "\n",
        "            for itemID, rec in enumerate(recs):\n",
        "                self.predictedRatings[uiid, itemID] = rec * 5.0\n",
        "\n",
        "        return self\n",
        "\n",
        "    def estimate(self, u, i):\n",
        "\n",
        "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
        "            raise PredictionImpossible('User and/or item is unkown.')\n",
        "\n",
        "        rating = self.predictedRatings[u, i]\n",
        "\n",
        "        if (rating < 0.001):\n",
        "            raise PredictionImpossible('No valid prediction exists.')\n",
        "\n",
        "        return rating\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import sys\n",
        "import re\n",
        "\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "class boardgameLens:\n",
        "\n",
        "    boardgameID_to_name = {}\n",
        "    name_to_boardgameID = {}\n",
        "\n",
        "    #ratingsPath =  '/content/drive/MyDrive/Colab Notebooks/Crawling/outcome/ratings.csv'\n",
        "    #boardgamesPath = '/content/drive/MyDrive/Colab Notebooks/Crawling/outcome/boardgames.csv'\n",
        "    ratingsPath = '/content/drive/MyDrive/Colab Notebooks/Crawling/outcome/userdata_rate_1021.csv'\n",
        "    boardgamesPath = '/content/drive/MyDrive/Colab Notebooks/Crawling/outcome/bg_data_3.csv'\n",
        "    def loadboardgameLensLatestSmall(self):\n",
        "\n",
        "        # Look for files relative to the directory we are running from\n",
        "        os.chdir(os.path.dirname(sys.argv[0]))\n",
        "\n",
        "        ratingsDataset = 0\n",
        "        self.boardgameID_to_name = {}\n",
        "        self.name_to_boardgameID = {}\n",
        "\n",
        "        reader = Reader(line_format='user item rating timestamp', sep='\\t', skip_lines=1)\n",
        "\n",
        "        ratingsDataset = Dataset.load_from_file(self.ratingsPath, reader=reader)\n",
        "\n",
        "        with open(self.boardgamesPath, newline='', encoding='UTF-8') as csvfile:\n",
        "                boardgameReader = csv.reader(csvfile)\n",
        "                next(boardgameReader)  #Skip header line\n",
        "                for row in boardgameReader:\n",
        "                    tmp_str = row[0].split(\"\\t\")\n",
        "                    boardgameID = int(tmp_str[0])\n",
        "                    boardgameName = tmp_str[1]\n",
        "                    self.boardgameID_to_name[boardgameID] = boardgameName\n",
        "                    self.name_to_boardgameID[boardgameName] = boardgameID\n",
        "\n",
        "        return ratingsDataset\n",
        "\n",
        "    def getUserRatings(self, user):\n",
        "        userRatings = []\n",
        "        hitUser = False\n",
        "        with open(self.ratingsPath, newline='') as csvfile:\n",
        "            ratingReader = csv.reader(csvfile)\n",
        "            next(ratingReader)\n",
        "            for row in ratingReader:\n",
        "                userID = int(row[0])\n",
        "                if (user == userID):\n",
        "                    boardgameID = int(row[1])\n",
        "                    rating = float(row[2])\n",
        "                    userRatings.append((boardgameID, rating))\n",
        "                    hitUser = True\n",
        "                if (hitUser and (user != userID)):\n",
        "                    break\n",
        "\n",
        "        return userRatings\n",
        "\n",
        "    def getPopularityRanks(self):\n",
        "        ratings = defaultdict(int)\n",
        "        rankings = defaultdict(int)\n",
        "        with open(self.ratingsPath, newline='') as csvfile:\n",
        "            ratingReader = csv.reader(csvfile)\n",
        "            next(ratingReader)\n",
        "            for row in ratingReader:\n",
        "                tmp_str = row[0].split(\"\\t\")\n",
        "                boardgameID = int(tmp_str[1])\n",
        "                ratings[boardgameID] += 1\n",
        "        rank = 1\n",
        "        for boardgameID, ratingCount in sorted(ratings.items(), key=lambda x: x[1], reverse=True):\n",
        "            rankings[boardgameID] = rank\n",
        "            rank += 1\n",
        "        return rankings\n",
        "\n",
        "    def getGenres(self):\n",
        "        genres = defaultdict(list)\n",
        "        genreIDs = {}\n",
        "        maxGenreID = 0\n",
        "        with open(self.boardgamesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "            boardgameReader = csv.reader(csvfile)\n",
        "            next(boardgameReader)  #Skip header line\n",
        "            for row in boardgameReader:\n",
        "                boardgameID = int(row[0])\n",
        "                genreList = row[2].split('|')\n",
        "                genreIDList = []\n",
        "                for genre in genreList:\n",
        "                    if genre in genreIDs:\n",
        "                        genreID = genreIDs[genre]\n",
        "                    else:\n",
        "                        genreID = maxGenreID\n",
        "                        genreIDs[genre] = genreID\n",
        "                        maxGenreID += 1\n",
        "                    genreIDList.append(genreID)\n",
        "                genres[boardgameID] = genreIDList\n",
        "        # Convert integer-encoded genre lists to bitfields that we can treat as vectors\n",
        "        for (boardgameID, genreIDList) in genres.items():\n",
        "            bitfield = [0] * maxGenreID\n",
        "            for genreID in genreIDList:\n",
        "                bitfield[genreID] = 1\n",
        "            genres[boardgameID] = bitfield\n",
        "\n",
        "        return genres\n",
        "\n",
        "    def getYears(self):\n",
        "        p = re.compile(r\"(?:\\((\\d{4})\\))?\\s*$\")\n",
        "        years = defaultdict(int)\n",
        "        with open(self.boardgamesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "            boardgameReader = csv.reader(csvfile)\n",
        "            next(boardgameReader)\n",
        "            for row in boardgameReader:\n",
        "                boardgameID = int(row[0])\n",
        "                title = row[1]\n",
        "                m = p.search(title)\n",
        "                year = m.group(1)\n",
        "                if year:\n",
        "                    years[boardgameID] = int(year)\n",
        "        return years\n",
        "\n",
        "    def getMiseEnScene(self):\n",
        "        mes = defaultdict(list)\n",
        "        with open(\"LLVisualFeatures13K_Log.csv\", newline='') as csvfile:\n",
        "            mesReader = csv.reader(csvfile)\n",
        "            next(mesReader)\n",
        "            for row in mesReader:\n",
        "                boardgameID = int(row[0])\n",
        "                avgShotLength = float(row[1])\n",
        "                meanColorVariance = float(row[2])\n",
        "                stddevColorVariance = float(row[3])\n",
        "                meanMotion = float(row[4])\n",
        "                stddevMotion = float(row[5])\n",
        "                meanLightingKey = float(row[6])\n",
        "                numShots = float(row[7])\n",
        "                mes[boardgameID] = [avgShotLength, meanColorVariance, stddevColorVariance,\n",
        "                   meanMotion, stddevMotion, meanLightingKey, numShots]\n",
        "        return mes\n",
        "\n",
        "    def getboardgameName(self, boardgameID):\n",
        "        if boardgameID in self.boardgameID_to_name:\n",
        "            return self.boardgameID_to_name[boardgameID]\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "    def getboardgameID(self, boardgameName):\n",
        "        if boardgameName in self.name_to_boardgameID:\n",
        "            return self.name_to_boardgameID[boardgameName]\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise.model_selection import LeaveOneOut\n",
        "from surprise import KNNBaseline\n",
        "\n",
        "class EvaluationData:\n",
        "\n",
        "    def __init__(self, data, popularityRankings):\n",
        "\n",
        "        self.rankings = popularityRankings\n",
        "\n",
        "        #Build a full training set for evaluating overall properties\n",
        "        self.fullTrainSet = data.build_full_trainset()\n",
        "        self.fullAntiTestSet = self.fullTrainSet.build_anti_testset()\n",
        "\n",
        "        #Build a 75/25 train/test split for measuring accuracy\n",
        "        self.trainSet, self.testSet = train_test_split(data, test_size=.25, random_state=1)\n",
        "\n",
        "        #Build a \"leave one out\" train/test split for evaluating top-N recommenders\n",
        "        #And build an anti-test-set for building predictions\n",
        "        LOOCV = LeaveOneOut(n_splits=1, random_state=1)\n",
        "        for train, test in LOOCV.split(data):\n",
        "            self.LOOCVTrain = train\n",
        "            self.LOOCVTest = test\n",
        "\n",
        "        self.LOOCVAntiTestSet = self.LOOCVTrain.build_anti_testset()\n",
        "\n",
        "        #Compute similarty matrix between items so we can measure diversity\n",
        "        sim_options = {'name': 'cosine', 'user_based': False}\n",
        "        self.simsAlgo = KNNBaseline(sim_options=sim_options)\n",
        "        self.simsAlgo.fit(self.fullTrainSet)\n",
        "\n",
        "    def GetFullTrainSet(self):\n",
        "        return self.fullTrainSet\n",
        "\n",
        "    def GetFullAntiTestSet(self):\n",
        "        return self.fullAntiTestSet\n",
        "\n",
        "    def GetAntiTestSetForUser(self, testSubject):\n",
        "        trainset = self.fullTrainSet\n",
        "        fill = trainset.global_mean\n",
        "        anti_testset = []\n",
        "        u = trainset.to_inner_uid(str(testSubject))\n",
        "        user_items = set([j for (j, _) in trainset.ur[u]])\n",
        "        anti_testset += [(trainset.to_raw_uid(u), trainset.to_raw_iid(i), fill) for\n",
        "                                 i in trainset.all_items() if\n",
        "                                 i not in user_items]\n",
        "        return anti_testset\n",
        "\n",
        "    def GetTrainSet(self):\n",
        "        return self.trainSet\n",
        "\n",
        "    def GetTestSet(self):\n",
        "        return self.testSet\n",
        "\n",
        "    def GetLOOCVTrainSet(self):\n",
        "        return self.LOOCVTrain\n",
        "\n",
        "    def GetLOOCVTestSet(self):\n",
        "        return self.LOOCVTest\n",
        "\n",
        "    def GetLOOCVAntiTestSet(self):\n",
        "        return self.LOOCVAntiTestSet\n",
        "\n",
        "    def GetSimilarities(self):\n",
        "        return self.simsAlgo\n",
        "\n",
        "    def GetPopularityRankings(self):\n",
        "        return self.rankings\n",
        "\n",
        "class Evaluator:\n",
        "\n",
        "    algorithms = []\n",
        "\n",
        "    def __init__(self, dataset, rankings):\n",
        "        ed = EvaluationData(dataset, rankings)\n",
        "        self.dataset = ed\n",
        "\n",
        "    def AddAlgorithm(self, algorithm, name):\n",
        "        alg = EvaluatedAlgorithm(algorithm, name)\n",
        "        self.algorithms.append(alg)\n",
        "\n",
        "    def Evaluate(self, doTopN):\n",
        "        results = {}\n",
        "        for algorithm in self.algorithms:\n",
        "            print(\"Evaluating \", algorithm.GetName(), \"...\")\n",
        "            results[algorithm.GetName()] = algorithm.Evaluate(self.dataset, doTopN)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if (doTopN):\n",
        "            print(\"{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "                    \"Algorithm\", \"RMSE\", \"MAE\", \"HR\", \"cHR\", \"ARHR\", \"Coverage\", \"Diversity\", \"Novelty\"))\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
        "                        name, metrics[\"RMSE\"], metrics[\"MAE\"], metrics[\"HR\"], metrics[\"cHR\"], metrics[\"ARHR\"],\n",
        "                                      metrics[\"Coverage\"], metrics[\"Diversity\"], metrics[\"Novelty\"]))\n",
        "        else:\n",
        "            print(\"{:<10} {:<10} {:<10}\".format(\"Algorithm\", \"RMSE\", \"MAE\"))\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f}\".format(name, metrics[\"RMSE\"], metrics[\"MAE\"]))\n",
        "\n",
        "        print(\"\\nLegend:\\n\")\n",
        "        print(\"RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\")\n",
        "        print(\"MAE:       Mean Absolute Error. Lower values mean better accuracy.\")\n",
        "        if (doTopN):\n",
        "            print(\"HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\")\n",
        "            print(\"cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\")\n",
        "            print(\"ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\" )\n",
        "            print(\"Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\")\n",
        "            print(\"Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\")\n",
        "            print(\"           for a given user. Higher means more diverse.\")\n",
        "            print(\"Novelty:   Average popularity rank of recommended items. Higher means more novel.\")\n",
        "\n",
        "    def SampleTopNRecs(self, ml, testSubject=85, k=10):\n",
        "\n",
        "        for algo in self.algorithms:\n",
        "            print(\"\\nUsing recommender \", algo.GetName())\n",
        "\n",
        "            print(\"\\nBuilding recommendation model...\")\n",
        "            trainSet = self.dataset.GetFullTrainSet()\n",
        "            algo.GetAlgorithm().fit(trainSet)\n",
        "\n",
        "            print(\"Computing recommendations...\")\n",
        "            testSet = self.dataset.GetAntiTestSetForUser(testSubject)\n",
        "\n",
        "            predictions = algo.GetAlgorithm().test(testSet)\n",
        "\n",
        "            recommendations = []\n",
        "\n",
        "            print (\"\\nWe recommend:\")\n",
        "            for userID, boardgameID, actualRating, estimatedRating, _ in predictions:\n",
        "                intboardgameID = int(boardgameID)\n",
        "                recommendations.append((intboardgameID, estimatedRating))\n",
        "\n",
        "            recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            for ratings in recommendations[:10]:\n",
        "                print(ml.getboardgameName(ratings[0]), ratings[1])\n",
        "\n",
        "import itertools\n",
        "\n",
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "\n",
        "class RecommenderMetrics:\n",
        "\n",
        "    def MAE(predictions):\n",
        "        return accuracy.mae(predictions, verbose=False)\n",
        "\n",
        "    def RMSE(predictions):\n",
        "        return accuracy.rmse(predictions, verbose=False)\n",
        "\n",
        "    def GetTopN(predictions, n=10, minimumRating=0.0):\n",
        "        topN = defaultdict(list)\n",
        "\n",
        "        for userID, boardgameID, actualRating, estimatedRating, _ in predictions:\n",
        "            if (estimatedRating >= minimumRating):\n",
        "                topN[int(userID)].append((int(boardgameID), estimatedRating))\n",
        "\n",
        "        for userID, ratings in topN.items():\n",
        "            ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "            topN[int(userID)] = ratings[:n]\n",
        "\n",
        "        return topN\n",
        "\n",
        "    def HitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for leftOut in leftOutPredictions:\n",
        "            userID = leftOut[0]\n",
        "            leftOutboardgameID = leftOut[1]\n",
        "            # Is it in the predicted top 10 for this user?\n",
        "            hit = False\n",
        "            for boardgameID, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOutboardgameID) == int(boardgameID)):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits += 1\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "    def CumulativeHitRate(topNPredicted, leftOutPredictions, ratingCutoff=0):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutboardgameID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Only look at ability to recommend things the users actually liked...\n",
        "            if (actualRating >= ratingCutoff):\n",
        "                # Is it in the predicted top 10 for this user?\n",
        "                hit = False\n",
        "                for boardgameID, predictedRating in topNPredicted[int(userID)]:\n",
        "                    if (int(leftOutboardgameID) == boardgameID):\n",
        "                        hit = True\n",
        "                        break\n",
        "                if (hit) :\n",
        "                    hits += 1\n",
        "\n",
        "                total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "    def RatingHitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = defaultdict(float)\n",
        "        total = defaultdict(float)\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutboardgameID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hit = False\n",
        "            for boardgameID, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOutboardgameID) == boardgameID):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits[actualRating] += 1\n",
        "\n",
        "            total[actualRating] += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        for rating in sorted(hits.keys()):\n",
        "            print (rating, hits[rating] / total[rating])\n",
        "\n",
        "    def AverageReciprocalHitRank(topNPredicted, leftOutPredictions):\n",
        "        summation = 0\n",
        "        total = 0\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutboardgameID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hitRank = 0\n",
        "            rank = 0\n",
        "            for boardgameID, predictedRating in topNPredicted[int(userID)]:\n",
        "                rank = rank + 1\n",
        "                if (int(leftOutboardgameID) == boardgameID):\n",
        "                    hitRank = rank\n",
        "                    break\n",
        "            if (hitRank > 0) :\n",
        "                summation += 1.0 / hitRank\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        return summation / total\n",
        "\n",
        "    # What percentage of users have at least one \"good\" recommendation\n",
        "    def UserCoverage(topNPredicted, numUsers, ratingThreshold=0):\n",
        "        hits = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            hit = False\n",
        "            for boardgameID, predictedRating in topNPredicted[userID]:\n",
        "                if (predictedRating >= ratingThreshold):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit):\n",
        "                hits += 1\n",
        "\n",
        "        return hits / numUsers\n",
        "\n",
        "    def Diversity(topNPredicted, simsAlgo):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        simsMatrix = simsAlgo.compute_similarities()\n",
        "        for userID in topNPredicted.keys():\n",
        "            pairs = itertools.combinations(topNPredicted[userID], 2)\n",
        "            for pair in pairs:\n",
        "                boardgame1 = pair[0][0]\n",
        "                boardgame2 = pair[1][0]\n",
        "                innerID1 = simsAlgo.trainset.to_inner_iid(str(boardgame1))\n",
        "                innerID2 = simsAlgo.trainset.to_inner_iid(str(boardgame2))\n",
        "                similarity = simsMatrix[innerID1][innerID2]\n",
        "                total += similarity\n",
        "                n += 1\n",
        "\n",
        "        if (n > 0):\n",
        "            S = total / n\n",
        "            return (1-S)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def Novelty(topNPredicted, rankings):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            for rating in topNPredicted[userID]:\n",
        "                boardgameID = rating[0]\n",
        "                rank = rankings[boardgameID]\n",
        "                total += rank\n",
        "                n += 1\n",
        "        return total / n\n",
        "\n",
        "class EvaluatedAlgorithm:\n",
        "\n",
        "    def __init__(self, algorithm, name):\n",
        "        self.algorithm = algorithm\n",
        "        self.name = name\n",
        "\n",
        "    def Evaluate(self, evaluationData, doTopN, n=5, verbose=True):\n",
        "        metrics = {}\n",
        "        # Compute accuracy\n",
        "        if (verbose):\n",
        "            print(\"Evaluating accuracy...\")\n",
        "        self.algorithm.fit(evaluationData.GetTrainSet())\n",
        "        predictions = self.algorithm.test(evaluationData.GetTestSet())\n",
        "        metrics[\"RMSE\"] = RecommenderMetrics.RMSE(predictions)\n",
        "        metrics[\"MAE\"] = RecommenderMetrics.MAE(predictions)\n",
        "\n",
        "        if (doTopN):\n",
        "            # Evaluate top-10 with Leave One Out testing\n",
        "            if (verbose):\n",
        "                print(\"Evaluating top-N with leave-one-out...\")\n",
        "            self.algorithm.fit(evaluationData.GetLOOCVTrainSet())\n",
        "            leftOutPredictions = self.algorithm.test(evaluationData.GetLOOCVTestSet())\n",
        "            # Build predictions for all ratings not in the training set\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetLOOCVAntiTestSet())\n",
        "            # Compute top 10 recs for each user\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Computing hit-rate and rank metrics...\")\n",
        "            # See how often we recommended a boardgame the user actually rated\n",
        "            metrics[\"HR\"] = RecommenderMetrics.HitRate(topNPredicted, leftOutPredictions)\n",
        "            # See how often we recommended a boardgame the user actually liked\n",
        "            metrics[\"cHR\"] = RecommenderMetrics.CumulativeHitRate(topNPredicted, leftOutPredictions)\n",
        "            # Compute ARHR\n",
        "            metrics[\"ARHR\"] = RecommenderMetrics.AverageReciprocalHitRank(topNPredicted, leftOutPredictions)\n",
        "\n",
        "            #Evaluate properties of recommendations on full training set\n",
        "            if (verbose):\n",
        "                print(\"Computing recommendations with full data set...\")\n",
        "            self.algorithm.fit(evaluationData.GetFullTrainSet())\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetFullAntiTestSet())\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Analyzing coverage, diversity, and novelty...\")\n",
        "            # Print user coverage with a minimum predicted rating of 4.0:\n",
        "            metrics[\"Coverage\"] = RecommenderMetrics.UserCoverage(  topNPredicted,\n",
        "                                                                   evaluationData.GetFullTrainSet().n_users,\n",
        "                                                                   ratingThreshold=4.0)\n",
        "            # Measure diversity of recommendations:\n",
        "            metrics[\"Diversity\"] = RecommenderMetrics.Diversity(topNPredicted, evaluationData.GetSimilarities())\n",
        "\n",
        "            # Measure novelty (average popularity rank of recommendations):\n",
        "            metrics[\"Novelty\"] = RecommenderMetrics.Novelty(topNPredicted,\n",
        "                                                            evaluationData.GetPopularityRankings())\n",
        "\n",
        "        if (verbose):\n",
        "            print(\"Analysis complete.\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def GetName(self):\n",
        "        return self.name\n",
        "\n",
        "    def GetAlgorithm(self):\n",
        "        return self.algorithm\n",
        "\n",
        "from surprise import NormalPredictor\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def LoadboardgameLensData():\n",
        "    ml = boardgameLens()\n",
        "    print(\"Loading boardgame ratings...\")\n",
        "    data = ml.loadboardgameLensLatestSmall()\n",
        "    print(\"\\nComputing boardgame popularity ranks so we can measure novelty later...\")\n",
        "    rankings = ml.getPopularityRanks()\n",
        "    return (ml, data, rankings)\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Load up common data set for the recommender algorithms\n",
        "(ml, evaluationData, rankings) = LoadboardgameLensData()\n",
        "\n",
        "# Construct an Evaluator to, you know, evaluate them\n",
        "evaluator = Evaluator(evaluationData, rankings)\n",
        "\n",
        "#Autoencoder\n",
        "AutoRecSys = AutoRecAlgorithm()\n",
        "evaluator.AddAlgorithm(AutoRecSys, \"AutoRec\")\n",
        "\n",
        "# Just make random recommendations\n",
        "Random = NormalPredictor()\n",
        "evaluator.AddAlgorithm(Random, \"Random\")\n",
        "\n",
        "# Fight!\n",
        "evaluator.Evaluate(True)\n",
        "\n",
        "evaluator.SampleTopNRecs(ml)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wgKXpQfCQus",
        "outputId": "0759ae81-92c4-401a-8142-3d96fa36c435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading movie ratings...\n",
            "\n",
            "Computing movie popularity ranks so we can measure novelty later...\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Evaluating  AutoRec ...\n",
            "Evaluating accuracy...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Trained epoch  20\n",
            "Trained epoch  21\n",
            "Trained epoch  22\n",
            "Trained epoch  23\n",
            "Trained epoch  24\n",
            "Trained epoch  25\n",
            "Trained epoch  26\n",
            "Trained epoch  27\n",
            "Trained epoch  28\n",
            "Trained epoch  29\n",
            "Trained epoch  30\n",
            "Trained epoch  31\n",
            "Trained epoch  32\n",
            "Trained epoch  33\n",
            "Trained epoch  34\n",
            "Trained epoch  35\n",
            "Trained epoch  36\n",
            "Trained epoch  37\n",
            "Trained epoch  38\n",
            "Trained epoch  39\n",
            "Trained epoch  40\n",
            "Trained epoch  41\n",
            "Trained epoch  42\n",
            "Trained epoch  43\n",
            "Trained epoch  44\n",
            "Trained epoch  45\n",
            "Trained epoch  46\n",
            "Trained epoch  47\n",
            "Trained epoch  48\n",
            "Trained epoch  49\n",
            "Trained epoch  50\n",
            "Trained epoch  51\n",
            "Trained epoch  52\n",
            "Trained epoch  53\n",
            "Trained epoch  54\n",
            "Trained epoch  55\n",
            "Trained epoch  56\n",
            "Trained epoch  57\n",
            "Trained epoch  58\n",
            "Trained epoch  59\n",
            "Trained epoch  60\n",
            "Trained epoch  61\n",
            "Trained epoch  62\n",
            "Trained epoch  63\n",
            "Trained epoch  64\n",
            "Trained epoch  65\n",
            "Trained epoch  66\n",
            "Trained epoch  67\n",
            "Trained epoch  68\n",
            "Trained epoch  69\n",
            "Trained epoch  70\n",
            "Trained epoch  71\n",
            "Trained epoch  72\n",
            "Trained epoch  73\n",
            "Trained epoch  74\n",
            "Trained epoch  75\n",
            "Trained epoch  76\n",
            "Trained epoch  77\n",
            "Trained epoch  78\n",
            "Trained epoch  79\n",
            "Trained epoch  80\n",
            "Trained epoch  81\n",
            "Trained epoch  82\n",
            "Trained epoch  83\n",
            "Trained epoch  84\n",
            "Trained epoch  85\n",
            "Trained epoch  86\n",
            "Trained epoch  87\n",
            "Trained epoch  88\n",
            "Trained epoch  89\n",
            "Trained epoch  90\n",
            "Trained epoch  91\n",
            "Trained epoch  92\n",
            "Trained epoch  93\n",
            "Trained epoch  94\n",
            "Trained epoch  95\n",
            "Trained epoch  96\n",
            "Trained epoch  97\n",
            "Trained epoch  98\n",
            "Trained epoch  99\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Processing user  650\n",
            "Processing user  700\n",
            "Processing user  750\n",
            "Processing user  800\n",
            "Processing user  850\n",
            "Processing user  900\n",
            "Processing user  950\n",
            "Evaluating top-N with leave-one-out...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Trained epoch  20\n",
            "Trained epoch  21\n",
            "Trained epoch  22\n",
            "Trained epoch  23\n",
            "Trained epoch  24\n",
            "Trained epoch  25\n",
            "Trained epoch  26\n",
            "Trained epoch  27\n",
            "Trained epoch  28\n",
            "Trained epoch  29\n",
            "Trained epoch  30\n",
            "Trained epoch  31\n",
            "Trained epoch  32\n",
            "Trained epoch  33\n",
            "Trained epoch  34\n",
            "Trained epoch  35\n",
            "Trained epoch  36\n",
            "Trained epoch  37\n",
            "Trained epoch  38\n",
            "Trained epoch  39\n",
            "Trained epoch  40\n",
            "Trained epoch  41\n",
            "Trained epoch  42\n",
            "Trained epoch  43\n",
            "Trained epoch  44\n",
            "Trained epoch  45\n",
            "Trained epoch  46\n",
            "Trained epoch  47\n",
            "Trained epoch  48\n",
            "Trained epoch  49\n",
            "Trained epoch  50\n",
            "Trained epoch  51\n",
            "Trained epoch  52\n",
            "Trained epoch  53\n",
            "Trained epoch  54\n",
            "Trained epoch  55\n",
            "Trained epoch  56\n",
            "Trained epoch  57\n",
            "Trained epoch  58\n",
            "Trained epoch  59\n",
            "Trained epoch  60\n",
            "Trained epoch  61\n",
            "Trained epoch  62\n",
            "Trained epoch  63\n",
            "Trained epoch  64\n",
            "Trained epoch  65\n",
            "Trained epoch  66\n",
            "Trained epoch  67\n",
            "Trained epoch  68\n",
            "Trained epoch  69\n",
            "Trained epoch  70\n",
            "Trained epoch  71\n",
            "Trained epoch  72\n",
            "Trained epoch  73\n",
            "Trained epoch  74\n",
            "Trained epoch  75\n",
            "Trained epoch  76\n",
            "Trained epoch  77\n",
            "Trained epoch  78\n",
            "Trained epoch  79\n",
            "Trained epoch  80\n",
            "Trained epoch  81\n",
            "Trained epoch  82\n",
            "Trained epoch  83\n",
            "Trained epoch  84\n",
            "Trained epoch  85\n",
            "Trained epoch  86\n",
            "Trained epoch  87\n",
            "Trained epoch  88\n",
            "Trained epoch  89\n",
            "Trained epoch  90\n",
            "Trained epoch  91\n",
            "Trained epoch  92\n",
            "Trained epoch  93\n",
            "Trained epoch  94\n",
            "Trained epoch  95\n",
            "Trained epoch  96\n",
            "Trained epoch  97\n",
            "Trained epoch  98\n",
            "Trained epoch  99\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Processing user  650\n",
            "Processing user  700\n",
            "Processing user  750\n",
            "Processing user  800\n",
            "Processing user  850\n",
            "Processing user  900\n",
            "Processing user  950\n",
            "Computing hit-rate and rank metrics...\n",
            "Computing recommendations with full data set...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Trained epoch  20\n",
            "Trained epoch  21\n",
            "Trained epoch  22\n",
            "Trained epoch  23\n",
            "Trained epoch  24\n",
            "Trained epoch  25\n",
            "Trained epoch  26\n",
            "Trained epoch  27\n",
            "Trained epoch  28\n",
            "Trained epoch  29\n",
            "Trained epoch  30\n",
            "Trained epoch  31\n",
            "Trained epoch  32\n",
            "Trained epoch  33\n",
            "Trained epoch  34\n",
            "Trained epoch  35\n",
            "Trained epoch  36\n",
            "Trained epoch  37\n",
            "Trained epoch  38\n",
            "Trained epoch  39\n",
            "Trained epoch  40\n",
            "Trained epoch  41\n",
            "Trained epoch  42\n",
            "Trained epoch  43\n",
            "Trained epoch  44\n",
            "Trained epoch  45\n",
            "Trained epoch  46\n",
            "Trained epoch  47\n",
            "Trained epoch  48\n",
            "Trained epoch  49\n",
            "Trained epoch  50\n",
            "Trained epoch  51\n",
            "Trained epoch  52\n",
            "Trained epoch  53\n",
            "Trained epoch  54\n",
            "Trained epoch  55\n",
            "Trained epoch  56\n",
            "Trained epoch  57\n",
            "Trained epoch  58\n",
            "Trained epoch  59\n",
            "Trained epoch  60\n",
            "Trained epoch  61\n",
            "Trained epoch  62\n",
            "Trained epoch  63\n",
            "Trained epoch  64\n",
            "Trained epoch  65\n",
            "Trained epoch  66\n",
            "Trained epoch  67\n",
            "Trained epoch  68\n",
            "Trained epoch  69\n",
            "Trained epoch  70\n",
            "Trained epoch  71\n",
            "Trained epoch  72\n",
            "Trained epoch  73\n",
            "Trained epoch  74\n",
            "Trained epoch  75\n",
            "Trained epoch  76\n",
            "Trained epoch  77\n",
            "Trained epoch  78\n",
            "Trained epoch  79\n",
            "Trained epoch  80\n",
            "Trained epoch  81\n",
            "Trained epoch  82\n",
            "Trained epoch  83\n",
            "Trained epoch  84\n",
            "Trained epoch  85\n",
            "Trained epoch  86\n",
            "Trained epoch  87\n",
            "Trained epoch  88\n",
            "Trained epoch  89\n",
            "Trained epoch  90\n",
            "Trained epoch  91\n",
            "Trained epoch  92\n",
            "Trained epoch  93\n",
            "Trained epoch  94\n",
            "Trained epoch  95\n",
            "Trained epoch  96\n",
            "Trained epoch  97\n",
            "Trained epoch  98\n",
            "Trained epoch  99\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Processing user  650\n",
            "Processing user  700\n",
            "Processing user  750\n",
            "Processing user  800\n",
            "Processing user  850\n",
            "Processing user  900\n",
            "Processing user  950\n",
            "Analyzing coverage, diversity, and novelty...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Analysis complete.\n",
            "Evaluating  Random ...\n",
            "Evaluating accuracy...\n",
            "Evaluating top-N with leave-one-out...\n",
            "Computing hit-rate and rank metrics...\n",
            "Computing recommendations with full data set...\n",
            "Analyzing coverage, diversity, and novelty...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Analysis complete.\n",
            "\n",
            "\n",
            "Algorithm  RMSE       MAE        HR         cHR        ARHR       Coverage   Diversity  Novelty   \n",
            "AutoRec    2.5464     1.8578     0.0092     0.0092     0.0047     1.0000     0.0544     298.1728  \n",
            "Random     0.8586     0.4976     0.0051     0.0051     0.0033     0.0153     0.0534     300.7796  \n",
            "\n",
            "Legend:\n",
            "\n",
            "RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\n",
            "MAE:       Mean Absolute Error. Lower values mean better accuracy.\n",
            "HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\n",
            "cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\n",
            "ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\n",
            "Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\n",
            "Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\n",
            "           for a given user. Higher means more diverse.\n",
            "Novelty:   Average popularity rank of recommended items. Higher means more novel.\n",
            "\n",
            "Using recommender  AutoRec\n",
            "\n",
            "Building recommendation model...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Trained epoch  20\n",
            "Trained epoch  21\n",
            "Trained epoch  22\n",
            "Trained epoch  23\n",
            "Trained epoch  24\n",
            "Trained epoch  25\n",
            "Trained epoch  26\n",
            "Trained epoch  27\n",
            "Trained epoch  28\n",
            "Trained epoch  29\n",
            "Trained epoch  30\n",
            "Trained epoch  31\n",
            "Trained epoch  32\n",
            "Trained epoch  33\n",
            "Trained epoch  34\n",
            "Trained epoch  35\n",
            "Trained epoch  36\n",
            "Trained epoch  37\n",
            "Trained epoch  38\n",
            "Trained epoch  39\n",
            "Trained epoch  40\n",
            "Trained epoch  41\n",
            "Trained epoch  42\n",
            "Trained epoch  43\n",
            "Trained epoch  44\n",
            "Trained epoch  45\n",
            "Trained epoch  46\n",
            "Trained epoch  47\n",
            "Trained epoch  48\n",
            "Trained epoch  49\n",
            "Trained epoch  50\n",
            "Trained epoch  51\n",
            "Trained epoch  52\n",
            "Trained epoch  53\n",
            "Trained epoch  54\n",
            "Trained epoch  55\n",
            "Trained epoch  56\n",
            "Trained epoch  57\n",
            "Trained epoch  58\n",
            "Trained epoch  59\n",
            "Trained epoch  60\n",
            "Trained epoch  61\n",
            "Trained epoch  62\n",
            "Trained epoch  63\n",
            "Trained epoch  64\n",
            "Trained epoch  65\n",
            "Trained epoch  66\n",
            "Trained epoch  67\n",
            "Trained epoch  68\n",
            "Trained epoch  69\n",
            "Trained epoch  70\n",
            "Trained epoch  71\n",
            "Trained epoch  72\n",
            "Trained epoch  73\n",
            "Trained epoch  74\n",
            "Trained epoch  75\n",
            "Trained epoch  76\n",
            "Trained epoch  77\n",
            "Trained epoch  78\n",
            "Trained epoch  79\n",
            "Trained epoch  80\n",
            "Trained epoch  81\n",
            "Trained epoch  82\n",
            "Trained epoch  83\n",
            "Trained epoch  84\n",
            "Trained epoch  85\n",
            "Trained epoch  86\n",
            "Trained epoch  87\n",
            "Trained epoch  88\n",
            "Trained epoch  89\n",
            "Trained epoch  90\n",
            "Trained epoch  91\n",
            "Trained epoch  92\n",
            "Trained epoch  93\n",
            "Trained epoch  94\n",
            "Trained epoch  95\n",
            "Trained epoch  96\n",
            "Trained epoch  97\n",
            "Trained epoch  98\n",
            "Trained epoch  99\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Processing user  400\n",
            "Processing user  450\n",
            "Processing user  500\n",
            "Processing user  550\n",
            "Processing user  600\n",
            "Processing user  650\n",
            "Processing user  700\n",
            "Processing user  750\n",
            "Processing user  800\n",
            "Processing user  850\n",
            "Processing user  900\n",
            "Processing user  950\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "캔트스탑 익스프레스 5\n",
            "슈퍼 판타지 브롤 5\n",
            "마스커레이드 5\n",
            "스카트 5\n",
            "비익 4.999999\n",
            "오리진 4.999999\n",
            "반디도 4.999999\n",
            "글로우 4.999997\n",
            "에퀴녹스 4.999996\n",
            "드래곤 라인 4.9999957\n",
            "\n",
            "Using recommender  Random\n",
            "\n",
            "Building recommendation model...\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "크라켄 업 2.98971401301607\n",
            "7 원더스: 건축가들 2.946085086212147\n",
            "육목 2.8324913072222606\n",
            "어둠의 공작원 2.760971847188551\n",
            "무조건 항복 2.6920404660153405\n",
            "66번 국도 2.680557144074319\n",
            "유나이티드 스퀘어 2.680210539443369\n",
            "K2 2.6754274224471244\n",
            "반디도 2.65373557984638\n",
            "롤드 웨스트 2.6338457085328484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB0uhd1GKz6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c426210-d2b4-41ae-e47d-63aa2c52d06b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected boardgames (5boardgames) : 럭키 넘버스,콰르토,도블,버글 브라더스,롤 & 범프\n",
            "\n",
            "\n",
            "   user_num  board_game  rate  title\n",
            "0         1          29     5  태피스트리\n",
            "1         2          29     1  태피스트리\n",
            "2         3          29     1  태피스트리\n",
            "3         4          29     1  태피스트리\n",
            "4         6          29     5  태피스트리\n",
            "title     13클루  218고지 전투  66번 국도  7 원더스  7 원더스: 건축가들  7 원더스: 대결  \\\n",
            "user_num                                                          \n",
            "1            1         1       1      1            1          0   \n",
            "2            1         1       0      1            0          0   \n",
            "3            1         1       0      1            0          1   \n",
            "4            1         1       1      1            1          1   \n",
            "5            0         0       0      1            1          1   \n",
            "...        ...       ...     ...    ...          ...        ...   \n",
            "975          1         0       0      0            0          0   \n",
            "976          0         0       0      0            0          0   \n",
            "977          0         0       0      0            0          0   \n",
            "978          1         1       0      1            0          1   \n",
            "979          0         0       0      0            0          0   \n",
            "\n",
            "title     99 (트릭테이킹 카드 게임)  99 (합산 카드 게임)  EVL  K2  ...  헬로프  호더즈  홈스테더스  홈월즈  \\\n",
            "user_num                                            ...                         \n",
            "1                        0              1    0   0  ...    0    1      1    1   \n",
            "2                        0              1    0   1  ...    1    1      0    0   \n",
            "3                        0              1    0   0  ...    1    0      1    0   \n",
            "4                        1              1    1   1  ...    1    1      1    1   \n",
            "5                        0              1    0   0  ...    0    0      0    0   \n",
            "...                    ...            ...  ...  ..  ...  ...  ...    ...  ...   \n",
            "975                      0              0    0   0  ...    0    0      0    0   \n",
            "976                      0              0    0   0  ...    0    0      0    0   \n",
            "977                      0              0    0   0  ...    0    0      0    0   \n",
            "978                      1              1    0   0  ...    0    1      0    1   \n",
            "979                      0              0    0   0  ...    0    0      0    0   \n",
            "\n",
            "title     화산 폭발  화성인 주사위  훌라  휘스트 22  휴고  힙노시아  \n",
            "user_num                                        \n",
            "1             0        0   1       1   1     1  \n",
            "2             0        1   1       1   1     0  \n",
            "3             0        1   1       0   0     1  \n",
            "4             1        1   1       1   1     1  \n",
            "5             0        1   0       0   0     1  \n",
            "...         ...      ...  ..     ...  ..   ...  \n",
            "975           0        0   0       0   0     0  \n",
            "976           0        1   0       0   0     0  \n",
            "977           0        1   0       0   0     0  \n",
            "978           0        1   0       0   1     0  \n",
            "979           0        0   0       0   0     0  \n",
            "\n",
            "[978 rows x 519 columns]\n",
            "frequent_item     support     itemsets\n",
            "0  0.779141      (7 원더스)\n",
            "1  0.332311  (7 원더스: 대결)\n",
            "2  0.463190   (가이아 프로젝트)\n",
            "3  0.351738       (갱스타!)\n",
            "4  0.331288       (국립공원)\n",
            "apriori_result:  ['스페이스 베이스', '야찌', '캔트 스탑', '킹 오브 도쿄', '스플렌더', '센추리: 향신료의 길', '모아보새', '로스트 시티', '하나비', '7 원더스', '레지사이드', '아줄', '아그리콜라']\n",
            "apriori_result:  ['스페이스 베이스', '야찌', '캔트 스탑', '킹 오브 도쿄', '스플렌더', '센추리: 향신료의 길', '모아보새', '로스트 시티', '하나비', '7 원더스', '레지사이드', '아줄', '아그리콜라']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      title  num_player_small  num_player_big  game_time(min)  complexity  \\\n",
            "0  티켓 투 라이드                 2               2              16           2   \n",
            "1       윙스팬                 2               2              55           3   \n",
            "2        아줄                 2               2              12           1   \n",
            "3        카탄                 3               3              42           2   \n",
            "4      카르카손                 2               2              18           1   \n",
            "\n",
            "   strategy  luck  interaction  cluster  \n",
            "0         3     3            2        4  \n",
            "1         3     3            3        4  \n",
            "2         3     3            0        0  \n",
            "3         3     3            4        4  \n",
            "4         3     3            3        4  \n",
            "cluster_candid\n",
            " :::: \n",
            " 0\n",
            "cluster_candid\n",
            " :::: \n",
            " 1\n",
            "cluster_candid\n",
            " :::: \n",
            " 4\n",
            "cluster_candid\n",
            " :::: \n",
            " 4\n",
            "cluster_candid\n",
            " :::: \n",
            " 0\n",
            "kmeans_result:  ['스페이스 베이스', '야찌', '캔트 스탑', '킹 오브 도쿄', '스플렌더', '센추리: 향신료의 길', '모아보새', '하나비', '7 원더스', '레지사이드', '아줄']\n",
            "Selected boardgames (5boardgames) : 럭키 넘버스,콰르토,도블,버글 브라더스,롤 & 범프\n",
            "\n",
            "A-priori & K-means clustering recommend boardgame : 스페이스 베이스,야찌,캔트 스탑,킹 오브 도쿄,스플렌더,센추리: 향신료의 길,모아보새,하나비,7 원더스,레지사이드,아줄\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from fuzzywuzzy import process\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import preprocessing\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "def ErrorLog(error):\n",
        "    current_time = time.strftime(\"%Y.%m.%d/%H:%M:%S\", time.localtime(time.time()))\n",
        "    with open(\"Log.txt\", \"a\") as f:\n",
        "        f.write(f\"[{current_time}] - {error}\\n\")\n",
        "\n",
        "def apriori_encoding(r): ###튀는 데이터 넣는 용도인가?\n",
        "    if r <= 0:\n",
        "        return 0\n",
        "    elif r >= 1:\n",
        "        return 1\n",
        "\n",
        "def do_apriori(_input_boardgames, _boardgames_df, _ratings_df):\n",
        "    # Internal variables\n",
        "    _apriori_result = []\n",
        "\n",
        "    \"\"\" Remove the Nan title & join the dataset \"\"\"\n",
        "    Nan_title = _boardgames_df['title'].isna()\n",
        "    _boardgames_df = _boardgames_df.loc[Nan_title == False]\n",
        "\n",
        "    df = pd.merge(_ratings_df, _boardgames_df[['id', 'title']], left_on='board_game', right_on='id')\n",
        "    df.drop(['timestamp', 'id'], axis=1, inplace=True)\n",
        "    print(df.head())\n",
        "    \"\"\" Prepare Apriori\n",
        "        row : userId | col : movies \"\"\"\n",
        "    df_pivot = df.pivot(index='user_num', columns='title', values='rate').fillna(0)\n",
        "  #  df_pivot = df_pivot.astype('int64')\n",
        "    df_pivot = df_pivot.applymap(apriori_encoding) ##apriori endoding용도는??\n",
        "    print(df_pivot)\n",
        "\n",
        "    \"\"\" A-priori Algorithm \"\"\"\n",
        "    #calculate support and eradicate under min_support\n",
        "    frequent_items = apriori(df_pivot, min_support=0.3, use_colnames=True)\n",
        "    print(\"frequent_item\" , frequent_items.head())\n",
        "\n",
        "    # using association rules, compute the other parameter ex) confidence, lift ..\n",
        "    association_indicator = association_rules(frequent_items, metric=\"lift\", min_threshold=1)\n",
        "\n",
        "    # sort by order of lift\n",
        "    df_lift = association_indicator.sort_values(by=['lift'], ascending=False)\n",
        "\n",
        "    \"\"\" Start recommendation \"\"\"\n",
        "    for selected_boardgames in _input_boardgames:\n",
        "        num = 0\n",
        "        df_selected = df_lift[df_lift['antecedents'].apply(lambda x: len(x) == 1 and next(iter(x)) == selected_boardgames)]\n",
        "        df_selected = df_selected[df_selected['lift'] > 1.2]\n",
        "        recommended_boardgames = df_selected['consequents'].values\n",
        "\n",
        "        for boardgames in recommended_boardgames:\n",
        "            for title in boardgames:\n",
        "                if title not in _apriori_result and num < 10:\n",
        "                    _apriori_result.append(title)\n",
        "                    num += 1\n",
        "\n",
        "    print(\"apriori_result: \", _apriori_result)\n",
        "    return _apriori_result\n",
        "\n",
        "def do_kmeans(_apriori_result, _input_boardgames, _boardgames_df):\n",
        "    # record all clusters in _input_movies\n",
        "    clusters = []\n",
        "    _kmeans_result = []\n",
        "\n",
        "    numeric_df = _boardgames_df[['title','num_player_small',\t'num_player_big',\t'game_time(min)',\t'complexity',\t'strategy',\t'luck',\t'interaction']]\n",
        "\n",
        "    numeric_df.isnull().sum()\n",
        "    numeric_df.dropna(inplace=True)\n",
        "\n",
        "    minmax_processed = preprocessing.MinMaxScaler().fit_transform(numeric_df.drop(['title'], axis=1))\n",
        "    df_numeric_scaled = pd.DataFrame(minmax_processed, index=numeric_df.index, columns=numeric_df.columns[:-1]) \n",
        "\n",
        "    \"\"\"Apply K-means clustering\"\"\"\n",
        "    # make elbow curve to determine value 'k'\n",
        "    num_cluster = range(1, 20)\n",
        "    kmeans = [KMeans(n_clusters=i) for i in num_cluster]\n",
        "    score = [kmeans[i].fit(df_numeric_scaled).score(df_numeric_scaled) for i in range(len(kmeans))]\n",
        "\n",
        "    # print elbow curve\n",
        "    pl.plot(num_cluster, score)\n",
        "    pl.xlabel(\"Number of clusters\")\n",
        "    pl.ylabel(\"Score\")\n",
        "    pl.title(\"Elbow curve\")\n",
        "    plt.show()  # maybe k=4 is appropriate\n",
        "\n",
        "    # Fit K-means clustering for k=5\n",
        "    kmeans = KMeans(n_clusters=5)\n",
        "    kmeans.fit(df_numeric_scaled)  # result is kmeans_label\n",
        "\n",
        "    # write back labels to the original numeric data frame\n",
        "    numeric_df['cluster'] = kmeans.labels_\n",
        "    print(numeric_df.head())\n",
        "\n",
        "    # Search all clusters in user selected movies\n",
        "    for boardgames1 in _input_boardgames:\n",
        "        try:\n",
        "            cluster_candid = numeric_df.loc[numeric_df[\"title\"] == boardgames1, 'cluster'].values[0]\n",
        "            print('cluster_candid\\n :::: \\n', cluster_candid)\n",
        "            clusters.append(cluster_candid)\n",
        "        except IndexError as e:\n",
        "            msg = \"There is No cluster in boardgames [\" + boardgames1 + ']'\n",
        "            ErrorLog(msg)\n",
        "            print(msg)\n",
        "\n",
        "    # Filtering movies that are not in clusters\n",
        "    for boardgames2 in _apriori_result:\n",
        "        try:\n",
        "            cluster_tmp = numeric_df.loc[numeric_df[\"title\"] == boardgames2, 'cluster'].values[0]\n",
        "            if cluster_tmp in clusters:\n",
        "                _kmeans_result.append(boardgames2)\n",
        "        except IndexError as e:\n",
        "            msg = \"There is No cluster in boardgames [\" + boardgames2 + ']'\n",
        "            ErrorLog(msg)\n",
        "            print(msg)\n",
        "\n",
        "    return _kmeans_result\n",
        "\n",
        "def compute_CF(_boardgames, _boardgames_users_compressed, _model, _n_recommendations, _boardgames_names):\n",
        "    \n",
        "    recommend_frame = []\n",
        "\n",
        "    _model.fit(_boardgames_users_compressed)\n",
        "    boardgames_index = process.extractOne(_boardgames, _boardgames_names['title'])[2]\n",
        "\n",
        "    try:\n",
        "        # find the closest movie compare to selected movie\n",
        "        distances, indices = _model.kneighbors(_boardgames_users_compressed[boardgames_index], n_neighbors=_n_recommendations)\n",
        "    except IndexError as e:\n",
        "        msg = \"There is No boardgame [\" + _boardgames + '] in csr_matrix'\n",
        "        print(msg)\n",
        "        ErrorLog(msg)\n",
        "        return []\n",
        "\n",
        "    rec_indices = sorted(list(zip(indices.squeeze().tolist(), distances.squeeze().tolist())),\n",
        "                                key=lambda x: x[1])[:0:-1]\n",
        "\n",
        "    check = 0 #######\n",
        "    for index in rec_indices:\n",
        "        check = check +1\n",
        "        print(check)\n",
        "        print(index[1])\n",
        "        try:\n",
        "          print( _boardgames_names['title'][index[0]])\n",
        "          recommend_frame.append({'Title': _boardgames_names['title'][index[0]], 'Distance': index[1]})\n",
        "        except: print(\"err1\")\n",
        "\n",
        "    df = pd.DataFrame(recommend_frame, index=range(1, _n_recommendations))\n",
        "    result = df['Title'].tolist()\n",
        "\n",
        "    return result\n",
        "\n",
        "def do_collaborative_filtering(_input_boardgames, _ratings_df, _boardgames_df):\n",
        "    \n",
        "    _collab_result = []\n",
        "    \n",
        "    boardgames_names = _boardgames_df[['title']]\n",
        "\n",
        "    boardgames_users = _ratings_df.pivot(index=['user_num'], columns=['board_game'], values='rate').fillna(0)\n",
        "\n",
        "    # by using csr_matrix, compress the sparse data frame\n",
        "    boardgames_users_compressed = csr_matrix(boardgames_users.values)\n",
        "    print(boardgames_users_compressed)\n",
        "\n",
        "    model = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=10, n_jobs=-1)\n",
        "\n",
        "    # make a similar movie groups based on ratings\n",
        "    model.fit(boardgames_users_compressed)\n",
        "\n",
        "    for boardgames in _input_boardgames:\n",
        "        num_recom = 4\n",
        "        recommend = compute_CF(boardgames, boardgames_users_compressed, model, num_recom, boardgames_names)\n",
        "        _collab_result.extend(recommend)\n",
        "\n",
        "    return _collab_result\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Function : main\n",
        "\n",
        "Parameter: _\n",
        "\n",
        "Return:\n",
        "        return : recommended movies\n",
        "\"\"\"\n",
        "def main(input_boardgames):\n",
        "\n",
        "    final_result = \"\"\n",
        "\n",
        "    final_result += \"Selected boardgames (5boardgames) : \" + \",\".join(input_boardgames) + \"\\n\\n\" ##입력한 결과 출력\n",
        "    print(final_result)\n",
        "\n",
        "    # Read csv files\n",
        "    boardgmae_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Crawling/outcome/boardgamedata_1021.csv', sep='\\t')\n",
        "    ratings_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Crawling/outcome/userdata_rate_1021.csv', sep='\\t')\n",
        "\n",
        "    # recommend based on a-priori & k-means\n",
        "    apriori_result = do_apriori(input_boardgames, boardgmae_df, ratings_df)\n",
        "    print(\"apriori_result: \", apriori_result)\n",
        "    kmeans_result = do_kmeans(apriori_result, input_boardgames, boardgmae_df)\n",
        "    print(\"kmeans_result: \", kmeans_result)\n",
        "\n",
        "    # recommend based on collaborative filtering\n",
        "    #collabo_result = do_collaborative_filtering(input_boardgames, ratings_df, boardgmae_df)\n",
        "    #print(\"collabo_result: \", collabo_result)\n",
        "\n",
        "    final_result += \"A-priori & K-means clustering recommend boardgame : \" + \",\".join(kmeans_result) + \"\\n\\n\"\n",
        "    #final_result += \"Collaborative filtering recommend boardgame : \" + \",\".join(collabo_result) + \"\\n\\n\"\n",
        "\n",
        "    print(final_result)\n",
        "    f = open(\"result.txt\", \"w\")\n",
        "    f.write(final_result)\n",
        "    f.close()\n",
        "\n",
        "    return final_result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(['럭키 넘버스', '콰르토', '도블', '버글 브라더스', '롤 & 범프'])"
      ]
    }
  ]
}